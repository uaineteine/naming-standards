from setuptools import setup, find_packages

requirements = ['polars', 'pandas', 'pyspark', 'sas-to-polars', 'sparkpolars', 'uainepydat', 'pyvis', 'networkx']
long_description = '# Meta Transforms Framework - Beta v0.2.1\n\nThis program provides a data transformation framework for working with tables (DataFrames) in PySpark, Pandas, or Polars. It tracks all transformations and important events in a structured, auditable way using JSON logs.\n\n# Main Components\n\n```\n|-- transformslib/\n|   |-- events/\n|   |   |-- jsonlog.py\n|   |   `-- eventlog.py\n|   |   `-- pipeevent.py\n|   |-- tables/\n|   |   |-- collections/\n|   |   |   |-- collection.py\n|   |   |   `-- supply_load.py\n|   |   |-- names/\n|   |   |   |-- headername.py\n|   |   |   |-- lists.py\n|   |   |   `-- tablename.py\n|   |   |-- metaframe.py\n|   |   `-- multitable.py\n|   |-- mapping/\n|   |   |-- maps.py\n|   |   `-- dag.py\n|   |-- transforms/\n|   |   |-- base.py\n|   |   |-- atomiclib.py\n|   |   |-- macrolib.py\n|   |   `-- reader.py\n|   |-- setup.py\n|   `-- meta.py\n`-- templates/\n    |-- template_load_pipe.py\n    `-- template_custom_transform.py\n```\n\n## Summary\n\nThis framework makes your data pipeline transparent and auditable by logging every important action as a structured JSON event. You can trace exactly what happened to each table, when, and why. The framework provides three main approaches for managing tables:\n\n1. **Direct table loading** with `MetaFrame` for single table operations\n2. **Payload-based loading** with `SupplyLoad` for configuration-driven multi-table loading\n3. **Collection management** with `Tablecollection` for flexible multi-table operations\n\nEach approach provides the same event tracking and audit capabilities while offering different levels of control and automation for your specific use case.\n\n## Naming\n\nThe naming module provides standardised classes for validating and managing table names, column headers, and variable lists. These utilities ensure consistent naming conventions across your data pipeline.\n\n### 1. Tablename\n**Purpose:** Validates and standardises table names according to database naming conventions.\n\n**Features:**\n- Enforces valid database table identifier rules\n- Allows purely numeric names (e.g., "123")\n- Requires non-numeric names to start with a letter or underscore\n- Permits only alphanumeric characters and underscores\n- Extends the built-in `str` class for seamless integration\n\n**Validation Rules:**\n- Must not be empty\n- Can be purely numeric (e.g., "123")\n- Otherwise, must start with a letter (a-z, A-Z) or underscore (_)\n- Must contain only alphanumeric characters and underscores\n- No spaces or special characters (except underscore)\n\n**Examples:**\n```python\nfrom transformslib.tables.names.tablename import Tablename\n\n# Valid table names\nvalid_names = [\n    Tablename("my_table"),        # Standard format\n    Tablename("_private_table"),  # Starting with underscore\n    Tablename("123"),             # Purely numeric\n    Tablename("table123"),        # Mixed alphanumeric\n    Tablename("CustomerData"),    # CamelCase\n]\n\n# Invalid table names (will raise ValueError)\ntry:\n    Tablename("1table")          # Cannot start with digit\nexcept ValueError as e:\n    print(e)\n\ntry:\n    Tablename("table-name")      # No hyphens allowed\nexcept ValueError as e:\n    print(e)\n\ntry:\n    Tablename("")                # Cannot be empty\nexcept ValueError as e:\n    print(e)\n```\n\n### 2. Headername\n**Purpose:** Validates and standardises column header names with strict formatting rules.\n\n**Features:**\n- Enforces uppercase-only column headers\n- Automatically converts input to uppercase\n- Allows only letters and digits\n- No spaces, symbols, or special characters permitted\n- Extends the built-in `str` class for seamless integration\n\n**Validation Rules:**\n- Must not be empty\n- Must contain only uppercase letters (A-Z) and digits (0-9)\n- No spaces, underscores, or special characters\n- Input is automatically converted to uppercase\n\n**Examples:**\n```python\nfrom transformslib.tables.names.headername import Headername\n\n# Valid header names\nvalid_headers = [\n    Headername("CUSTOMERNAME"),   # All caps letters\n    Headername("ORDER123"),       # Letters and numbers\n    Headername("customername"),   # Automatically converted to CUSTOMERNAME\n    Headername("order123"),       # Automatically converted to ORDER123\n]\n\n# Invalid header names (will raise ValueError)\ntry:\n    Headername("CUSTOMER_NAME")   # No underscores\nexcept ValueError as e:\n    print(e)\n\ntry:\n    Headername("ORDER DATE")      # No spaces\nexcept ValueError as e:\n    print(e)\n\ntry:\n    Headername("")                # Cannot be empty\nexcept ValueError as e:\n    print(e)\n```\n\n### 3. NamedList\n**Purpose:** A specialised list for managing collections of string names with automatic capitalisation.\n\n**Features:**\n- Extends built-in `list` functionality\n- Automatically converts all items to uppercase\n- Provides utility methods for list operations\n- JSON serialisation support\n- Set operations for overlap and extension\n\n**Examples:**\n```python\nfrom transformslib.tables.names.lists import NamedList\n\n# Create a named list\nvar_list = NamedList(["name", "age", "city"])\nprint(var_list)  # NamedList([\'NAME\', \'AGE\', \'CITY\'])\n\n# Properties and methods\nprint(f"Count: {var_list.count}")  # Count: 3\nprint(var_list.to_json())  # JSON representation\n\n# Set operations\nother_list = ["AGE", "SALARY", "DEPARTMENT"]\noverlap = var_list.overlap(other_list)\nprint(overlap)  # NamedList([\'AGE\'])\n\n# Extend with unique values\nvar_list.extend(other_list)\nprint(var_list)  # NamedList([\'NAME\', \'AGE\', \'CITY\', \'SALARY\', \'DEPARTMENT\'])\n```\n\n### 4. VarList\n**Purpose:** A validated list for variable names that must conform to header naming conventions.\n\n**Features:**\n- Extends `NamedList` functionality\n- Validates all items using `Headername` rules\n- Ensures all variable names are properly formatted\n- Raises `ValueError` for invalid formats\n- Automatic uppercase conversion\n\n**Examples:**\n```python\nfrom transformslib.tables.names.lists import VarList\n\n# Valid variable list\nvalid_vars = VarList(["customerid", "ordernumber", "amount123"])\nprint(valid_vars)  # VarList([\'CUSTOMERID\', \'ORDERNUMBER\', \'AMOUNT123\'])\n\n# Invalid variable list (will raise ValueError)\ntry:\n    invalid_vars = VarList(["customer_id", "order number", "amount"])\nexcept ValueError as e:\n    print(e)  # Column names must be in correct format\n\n# All NamedList methods are available\nprint(valid_vars.to_json())\nother_vars = ["PRODUCTID", "CUSTOMERID"]\noverlap = valid_vars.overlap(other_vars)\nprint(overlap)  # VarList([\'CUSTOMERID\'])\n```\n\n## Events\n\n### 1. EventLog\n**Purpose:** Handles event logging and saving of events during pipeline execution.\n\n**Features:**\n- Manages the logging of events generated during the data pipeline\'s execution.\n- Provides methods to save events to a JSON file for auditing and tracking purposes.\n- Supports different logging levels and destinations.\n\n![UML diagram](diagrams/events.png)\n\n### 2. PipelineEvent\n**Purpose:** Represents an event (e.g., loading a table, applying a transform).\n\n**Features:**\n- Stores detailed information about each event, including event type, message, description, timestamp, and a unique UUID.\n- Provides methods to format the event information as a JSON object.\n- Can be extended to represent specific types of events, such as load events or transform events.\n- Includes a log_location attribute to track where the event is logged.\n- Serves as a base class for all pipeline events, ensuring a consistent structure and interface.\n\n**Example JSON output for a load event:**\n```json\n{\n  "event_type": "load",\n  "message": "Loaded table from test.csv as csv (pyspark)",\n  "description": "Loaded test_table from test.csv",\n  "uuid": "b2e7c8e2-7d4e-4c7e-8b8e-2f6e7c8e2d4e",\n  "timestamp": "2025-08-10T12:34:56.789012",\n  "log_location": "events_log/job_1/test_table_events.json"\n}\n```\n\n## Tables\n\n![UML diagram](diagrams/tables.png)\n\n### 1. TableName\n**Purpose:** Represents a standardised name for a table.\n\n### 2. MultiTable\n**Purpose:** Manages multiple dataframes and tables in complex data pipelines.\n\n**Features:**\n- Stores and manages different types of dataframe objects.\n- Facilitates operations across multiple tables, such as applying the same transformation to all tables in the collection.\n- Provides methods for accessing and manipulating tables within the collection.\n\n### 3. Metaframe\n**Purpose:** Wraps a DataFrame and tracks its metadata.\n\n**Features:**\n- Supports loading tables from CSV, Parquet, or SAS files into Spark, Pandas, or Polars DataFrames.\n- Can convert between DataFrame types (Pandas, Polars, PySpark).\n- Stores table metadata including source path, table name, and frame type.\n- Provides utility methods for DataFrame type conversion.\n\n**Example:**\n```python\nfrom pyspark.sql import SparkSession\nfrom metaframe import MetaFrame\n\nspark = SparkSession.builder.getOrCreate()\ntbl = MetaFrame.load(spark=spark, path="test.csv", format="csv", table_name="test_table", frame_type="pyspark")\n```\n\n### 3. PipelineTable\n**Purpose:** Extends MetaFrame to include event tracking and logging capabilities.\n\n**Features:**\n- Inherits from MetaFrame and adds event tracking functionality.\n- Stores a list of `PipelineEvent` objects describing actions performed on the table.\n- Can save all events to a JSON log file.\n- Automatically logs load events when tables are loaded.\n\n**Example:**\n```python\nfrom pipeline_table import PipelineTable\n\ntbl = PipelineTable.load(spark=spark, path="test.csv", format="csv", table_name="test_table", frame_type="pyspark")\n```\n\n### 4. TableCollection\n**Purpose:** A collection manager for multiple PipelineTable objects with dictionary-like access.\n\n**Features:**\n- Manages multiple PipelineTable instances in a single collection.\n- Provides dictionary-style access to tables by name.\n- Supports adding, removing, and checking for tables.\n- Can save events for all tables in the collection at once.\n- Maintains both a list of tables and a dictionary for named access.\n- **NEW:** Advanced table selection by patterns, prefixes, suffixes, and ranges.\n- **NEW:** Returns references to the same table objects, so modifications are reflected across collections.\n\n**Example:**\n```python\nfrom pipeline_table import PipelineTable, PipelineTables\n\n# Load multiple tables\ntest_table = PipelineTable.load(spark=spark, path="test_tables/test.csv", format="csv", table_name="test_table", frame_type="pyspark")\ntest2_table = PipelineTable.load(spark=spark, path="test_tables/test2.csv", format="csv", table_name="test2_table", frame_type="pyspark")\nclus_table = PipelineTable.load(spark=spark, path="test_tables/clus_data.csv", format="csv", table_name="clus_data", frame_type="pyspark")\n\n# Create collection\ntables_list = [test_table, test2_table, clus_table]\npt_collection = PipelineTables(tables_list)\n\n# Access tables by name\nfirst_table = pt_collection["test_table"]\nprint(f"Collection has {pt_collection.ntables} tables")\nprint(f"Available tables: {list(pt_collection.named_tables.keys())}")\n\n# NEW: Select tables by patterns\nclus_tables = pt_collection.select_by_names("clus_*")  # All tables starting with "clus_"\nspecific_tables = pt_collection.select_by_names("test_table", "test2_table")  # Specific tables\nmixed_tables = pt_collection.select_by_names("clus_*", "test_table")  # Multiple patterns\n\n# NEW: Convenience methods\nprefix_tables = pt_collection.select_by_prefix("clus_")  # Same as "clus_*"\nsuffix_tables = pt_collection.select_by_suffix("_data")  # All tables ending with "_data"\nrange_tables = pt_collection.select_by_range("test_table", "test2_table")  # Tables in lexicographic range\n\n# NEW: Custom filtering\nlarge_tables = pt_collection.filter_tables(lambda t: len(t.df) > 1000)  # Tables with >1000 rows\n\n# Save events for all tables\npt_collection.save_events()\n```\n\n### 5. SupplyLoad\n**Purpose:** A specialised collection manager for loading and managing supply data from JSON configuration files.\n\n**Features:**\n- Extends PipelineTables to provide automated loading of multiple data sources.\n- Loads tables from a JSON configuration file.\n- Designed for scenarios where you need to load multiple related datasets from a single configuration source.\n- All tables are loaded as PySpark DataFrames by default.\n\n**Example:**\n```python\nfrom supply_load import SupplyLoad\n\n# Load multiple tables from JSON configuration\nsupply_frames = SupplyLoad("test_tables/payload.json", spark=spark)\nprint("Original columns:", supply_frames["test_table"].columns)\n\n# Apply transformations\nsupply_frames = DropVariable("age")(supply_frames, df="test_table")\n\n# Save events for all tables\nsupply_frames.save_events()\n```\n\n## Transforms\n\n![UML diagram](diagrams/transforms.png)\n\n### 1. Transform and Subclasses\n**Purpose:** Encapsulates data transformations.\n\n**Features:**\n- Each transform is a subclass of `PipelineEvent` and logs itself when applied, ensuring auditability.\n- `Transform` is the abstract base class for all transformations, defining the interface for transform execution.\n- `TableTransform` is a base class for transformations that operate on entire tables.\n- `SimpleTransform` is a base class for transformations that operate on a single variable or column within a table.\n- `DropVariable` is a concrete example, removing a specified column from a DataFrame.\n\n**Example:**\n```python\nfrom transforms import DropVariable\n\ntbl = DropVariable("age")(tbl)\n```\n\n## Example Workflows\n\n### Method 1: Direct Table Loading\n\n1. **Load a table:**\n    ```python\n    from pyspark.sql import SparkSession\n    from pipeline_table import PipelineTable\n    \n    spark = SparkSession.builder.master("local").appName("TransformTest").getOrCreate()\n    tbl = PipelineTable.load(spark=spark, path="test.csv", format="csv", table_name="test_table", frame_type="pyspark")\n    ```\n    Automatically logs a "load" event.\n\n2. **Apply a transformation:**\n    ```python\n    from transforms import DropVariable\n    \n    tbl = DropVariable("age")(tbl)\n    ```\n    Logs a "transform" event describing the column dropped.\n\n3. **Save all events:**\n    ```python\n    tbl.save_events()\n    ```\n    Writes all events to a JSON file for auditing.\n\n### Method 2: Payload-Based Loading (SupplyLoad)\n\nFor scenarios where you need to load multiple tables from a configuration file, you can use the `SupplyLoad` class with a JSON payload:\n\n1. **Create a payload configuration file** (`test_tables/payload.json`):\n    ```json\n    {\n      "job_id": 1,\n      "run_id": 2,\n      "supply": [\n        {\n          "name": "test_table",\n          "format": "csv",\n          "path": "test_tables/test.csv"\n        },\n        {\n          "name": "test_table2",\n          "format": "csv",\n          "path": "test_tables/test2.csv"\n        }\n      ]\n    }\n    ```\n\n2. **Load multiple tables using the payload:**\n    ```python\n    from pyspark.sql import SparkSession\n    from transforms import DropVariable\n    from supply_load import SupplyLoad\n\n    # Create Spark session\n    spark = SparkSession.builder.master("local").appName("TransformTest").getOrCreate()\n\n    # Load pipeline tables from payload\n    supply_frames = SupplyLoad("test_tables/payload.json", spark=spark)\n    print("Original columns:", supply_frames["test_table"].columns)\n\n    # Apply transformations to specific tables\n    supply_frames = DropVariable("age")(supply_frames, df="test_table")\n\n    # Show results\n    print("Transformed columns:", supply_frames["test_table"].columns)\n    supply_frames["test_table"].show()\n\n    # Save events for all tables\n    supply_frames.save_events()\n    ```\n\nThis approach is particularly useful when:\n- You need to load multiple related datasets\n- You want to configure your data sources externally\n- You need to manage complex data pipelines with many input sources\n- You want to version control your data source configurations separately from your code\n\n### Method 3: PipelineTables Collection\n\nFor managing multiple tables with more control and flexibility, use the `PipelineTables` collection:\n\n1. **Load multiple tables individually:**\n    ```python\n    from pyspark.sql import SparkSession\n    from transforms import DropVariable\n    from pipeline_table import PipelineTable, PipelineTables\n\n    # Create Spark session\n    spark = SparkSession.builder.master("local").appName("TransformTest").getOrCreate()\n\n    # Load multiple tables\n    test_table = PipelineTable.load(spark=spark, path="test_tables/test.csv", format="csv", table_name="test_table", frame_type="pyspark")\n    test2_table = PipelineTable.load(spark=spark, path="test_tables/test2.csv", format="csv", table_name="test2_table", frame_type="pyspark")\n    clus_table = PipelineTable.load(spark=spark, path="test_tables/clus_data.csv", format="csv", table_name="clus_data", frame_type="pyspark")\n    \n    # Create collection with initial tables\n    tables_list = [test_table, test2_table, clus_table]\n    pt_collection = PipelineTables(tables_list)\n    \n    print(f"Collection created with {pt_collection.ntables} tables")\n    print(f"Available tables: {list(pt_collection.named_tables.keys())}")\n    ```\n\n2. **Demonstrate collection operations:**\n    ```python\n    # Access tables by name\n    first_table = pt_collection["test_table"]\n    print(f"First table columns: {first_table.columns}")\n    \n    # Check if table exists\n    if "test2_table" in pt_collection:\n        print("test2_table exists in collection")\n    \n    # Apply transforms to specific tables\n    pt_collection = DropVariable("age")(pt_collection["test_table"])\n    \n    print(f"Transformed table columns: {pt_collection[\'test_table\'].columns}")\n    pt_collection["test_table"].show()\n    ```\n\n3. **NEW: Advanced table selection:**\n    ```python\n    # Select tables by patterns (returns references to same objects)\n    clus_tables = pt_collection.select_by_names("clus_*")\n    specific_tables = pt_collection.select_by_names("test_table", "test2_table")\n    \n    # Modifications in selected collection affect original\n    clus_tables["clus_data"] = DropVariable("unwanted_column")(clus_tables["clus_data"])\n    # This change is also reflected in pt_collection["clus_data"]\n    \n    # Convenience methods\n    prefix_tables = pt_collection.select_by_prefix("clus_")\n    suffix_tables = pt_collection.select_by_suffix("_data")\n    range_tables = pt_collection.select_by_range("test_table", "test2_table")\n    \n    # Custom filtering\n    large_tables = pt_collection.filter_tables(lambda t: len(t.df) > 1000)\n    ```\n\n4. **Show all tables and save events:**\n    ```python\n    # Show all tables in collection\n    for i, table in enumerate(pt_collection.tables):\n        print(f"Table {i+1}: {table.table_name} - Columns: {table.columns}")\n        table.df.show()\n    \n    # Save events for all tables in the collection\n    pt_collection.save_events()\n    ```\n\nThis approach provides:\n- Fine-grained control over table loading and management\n- Dictionary-style access to tables by name\n- Easy iteration over all tables in the collection\n- Bulk event saving for all tables\n- Flexibility to add/remove tables dynamically\n- **NEW:** Advanced table selection with pattern matching, wildcards, and custom filters\n- **NEW:** Shared object references ensuring modifications propagate across collections\n\n---\n\n## Advanced Table Selection Features\n\nThe `PipelineTables` class now includes powerful table selection capabilities that return references to the same table objects, ensuring modifications propagate across collections.\n\n### Table Selection Methods\n\n#### `select_by_names(*name_patterns)`\nSelect tables by name patterns, supporting wildcards and exact matches:\n\n```python\n# Wildcard patterns\nclus_tables = pt.select_by_names("clus_*")  # All tables starting with "clus_"\nyear_tables = pt.select_by_names("*_2023")  # All tables ending with "_2023"\ntable_tables = pt.select_by_names("table*")  # All tables starting with "table"\n\n# Exact matches\nspecific_tables = pt.select_by_names("table1", "table3", "clus_data")\n\n# Multiple patterns\nmixed_tables = pt.select_by_names("clus_*", "table1", "*_2023")\n```\n\n#### `select_by_prefix(prefix)`\nConvenience method for selecting tables that start with a specific prefix:\n\n```python\nclus_tables = pt.select_by_prefix("clus_")  # Same as "clus_*"\n```\n\n#### `select_by_suffix(suffix)`\nConvenience method for selecting tables that end with a specific suffix:\n\n```python\nyear_tables = pt.select_by_suffix("_2023")  # Same as "*_2023"\n```\n\n#### `select_by_range(start_name, end_name)`\nSelect tables with names that fall within a lexicographic range:\n\n```python\nrange_tables = pt.select_by_range("table1", "table5")  # Tables with names between table1 and table5\n```\n\n#### `filter_tables(filter_func)`\nAdvanced filtering using custom functions:\n\n```python\n# Filter tables with more than 1000 rows\nlarge_tables = pt.filter_tables(lambda t: len(t.df) > 1000)\n\n# Filter tables with specific column\ntables_with_age = pt.filter_tables(lambda t: "age" in t.columns)\n\n# Filter tables by custom criteria\nrecent_tables = pt.filter_tables(lambda t: "2023" in t.table_name)\n```\n\n### Shared Object References\n\n**Important:** All selection methods return references to the same `PipelineTable` objects. This means:\n\n```python\n# Create a collection\npt_collection = PipelineTables([table1, table2, clus_data])\n\n# Select a subset\nclus_tables = pt_collection.select_by_names("clus_*")\n\n# Modify a table in the subset\nclus_tables["clus_data"] = DropVariable("unwanted_column")(clus_tables["clus_data"])\n\n# The change is reflected in the original collection\nprint(pt_collection["clus_data"].columns)  # Shows the modified columns\n```\n\nThis behavior ensures that:\n- Modifications to tables in selected collections affect the original collection\n- Memory efficiency (no copying of table objects)\n- Consistent state across all collections referencing the same tables\n\n### Utility Methods\n\n#### `get_table_names()`\nGet a list of all table names in the collection:\n\n```python\nnames = pt.get_table_names()\nprint(names)  # [\'table1\', \'table2\', \'clus_data\', ...]\n```\n\n---\n\n## Event Logging\n\n- Events are saved in `events_log/job_1/{table_name}_events.json`.\n- Each event is a JSON object, one per line.\n- This provides a complete audit trail of all actions performed on each table.\n\n**Example JSON output for a transform event:**\n```json\n{\n  "event_type": "transform",\n  "uuid": "7184dc0a-3606-4360-908c-1f6032f44d77",\n  "timestamp": "2025-08-25T04:14:57.862503+00:00",\n  "excuted_user": "Daniel",\n  "sub_type": "pipeline_event",\n  "message": "DropVariable",\n  "description": "Removes specified variable(s) from a dataframe",\n  "name": "DropVariable",\n  "transform_type": "TableTransform",\n  "testable_transform": true,\n  "version_pyspark": "4.0.0",\n  "version_polars": "1.31.0",\n  "version_pandas": "2.3.0",\n  "version_python": "3.10.8",\n  "transform_id": "DropVar",\n  "target_tables": [\n    "test_table"\n  ],\n  "target_variables": [\n    "AGE"\n  ],\n  "created_variables": null,\n  "renamed_variables": null,\n  "deleted_variables": [\n    "AGE"\n  ],\n  "hashed_variables": null\n}\n```\n\n**Example JSON output for a load event:**\n```json\n{\n  "event_type": "load",\n  "uuid": "b2e7c8e2-7d4e-4c7e-8b8e-2f6e7c8e2d4e",\n  "timestamp": "2025-08-10T12:34:56.789012",\n  "log_location": "events_log/job_1/test_table_events.json",\n  "message": "Loaded table from test.csv as csv (pyspark)",\n  "description": "Loaded test_table from test.csv"\n}\n```\n\n---\n\n## File Structure\n\n- [`transformslib/tables/metaframe.py`](transformslib/tables/metaframe.py): MetaFrame class for DataFrame wrapping and type conversion\n- [`transformslib/events/pipeline_event.py`](transformslib/events/pipeline_event.py): PipelineTable and PipelineTables classes for event tracking and collection management\n- [`transformslib/events/eventlog.py`](transformslib/events/eventlog.py): Event and PipelineEvent classes for logging\n- [`transformslib/transforms/base.py`](transformslib/transforms/base.py): Transformation classes (Transform, TableTransform, SimpleTransform, DropVariable)\n- [`transformslib/tables/collections/supply_load.py`](transformslib/tables/collections/supply_load.py): SupplyLoad class for loading multiple tables from JSON configuration\n- [`transformslib/tables/names/tablename.py`](transformslib/tables/names/tablename.py): Tablename class for validating table names according to database conventions\n- [`transformslib/tables/names/headername.py`](transformslib/tables/names/headername.py): Headername class for validating column header names with strict formatting\n- [`transformslib/tables/names/lists.py`](transformslib/tables/names/lists.py): NamedList and VarList classes for managing collections of validated variable names\n- [`templates/template_atomic_transform.py`](templates/template_atomic_transform.py): Example demonstrating PipelineTables collection usage\n- [`templates/template_atomic_pipe.py`](templates/template_atomic_pipe.py): Example demonstrating SupplyLoad usage\n\n---\n\n## Notes\n\n- All transformations should inherit from `Transform` and implement the `transforms` method.\n- The framework is designed for transparency and auditability in data pipelines by logging every important action as a structured JSON event.\n- The `PipelineTable` class is the main entry point for users who want event tracking functionality.\n- `PipelineTables` provides collection management for multiple tables with dictionary-like access.\n- `SupplyLoad` extends `PipelineTables` to provide automated loading from JSON configuration files.\n\n---\n\n## Building\n\nPrerequisites:\n- Python 3.10 or newer\n- pip, setuptools, and wheel\n\nOn Windows PowerShell:\n\n```powershell\n\n# Ensure build tools are available\npip install -U setuptools wheel\n\n# If you have project dependencies, install them (optional for building)\npip install -r requirements.txt\n\n# Build the package\npython build.py\n```\n\nWhat the build script does (python build.py):\n1. Pre-renders setup.py\n   - Reads requirements.txt (if present) and inlines them into install_requires\n   - Reads readme.md as the long_description for the package\n   - Writes a fully-populated setup.py with the package metadata (name, version, etc.)\n2. Cleans previous artifacts\n   - Deletes the build/ and dist/ folders if they exist\n3. Builds distributions\n   - Runs: python setup.py sdist bdist_wheel\n   - Outputs artifacts to the dist/ directory (e.g., .tar.gz and .whl files)\n4. Builds documentation\n   - If a docs/ folder exists, changes into docs/ and executes builddocs.bat\n   - Returns to the project root when finished\n\nAfter a successful build, you should see the generated distribution files under `dist/`.\n'

setup(
    name="transformslib",
    version="0.2.1",
    author="",
    author_email="",
    description="A python package of a working transforms framework",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="",
    packages=find_packages(include=["transformslib", "transformslib.*"]),
    classifiers=[
        "Programming Language :: Python :: 3",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.10",
    install_requires=requirements
)
